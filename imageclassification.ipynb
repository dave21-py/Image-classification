{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLmokdBxrrp8h7PmMuIsFq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave21-py/Image-classification/blob/main/imageclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVPkYZi3677u",
        "outputId": "7a38cff9-5752-481f-8637-820cb49761e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (5.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of folders you want to remove\n",
        "folders_to_delete = [\"__MACOSX\", \"apple\", \"orange\", \"dataset\", \"old\"]\n",
        "\n",
        "# Loop and remove each folder if it exists\n",
        "import shutil, os\n",
        "\n",
        "for folder in folders_to_delete:\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "        print(f\"Deleted folder: {folder}\")\n",
        "    else:\n",
        "        print(f\"Folder not found: {folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odnKq0ciT6WW",
        "outputId": "76b05444-0c9f-41f0-875c-44a9c907afa0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted folder: __MACOSX\n",
            "Deleted folder: apple\n",
            "Deleted folder: orange\n",
            "Deleted folder: dataset\n",
            "Deleted folder: old\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R3tr2pbT8MpP",
        "outputId": "c9bdda94-66a6-4888-a0c9-62aea7be496a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: icrawler in /usr/local/lib/python3.12/dist-packages (0.6.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from icrawler) (4.13.5)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.12/dist-packages (from icrawler) (0.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from icrawler) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from icrawler) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from icrawler) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from icrawler) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from icrawler.builtin import BingImageCrawler\n",
        "\n",
        "def fetch_images_bing(keyword, folder, max_num=50):\n",
        "    print(f\"Searching for '{keyword}' via Bing …\")\n",
        "    crawler = BingImageCrawler(storage={\"root_dir\": folder})\n",
        "    crawler.crawl(keyword=keyword, max_num=max_num)\n",
        "    print(f\"Downloaded {max_num} images into '{folder}'.\")\n",
        "\n",
        "fetch_images_bing(\"orange fruit\", \"orange\", 50)\n",
        "print(\"Download complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x9cHcNik8VJP",
        "outputId": "28681cb9-1ec2-4e6c-db0b-a7c5e4a8c404"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for 'orange fruit' via Bing …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Oranges_-_whole-halved-segment.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 50 images into 'orange'.\n",
            "Download complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from icrawler.builtin import BingImageCrawler\n",
        "\n",
        "def fetch_images_bing(keyword, folder, max_num=50):\n",
        "    print(f\"Searching for '{keyword}' via Bing …\")\n",
        "    crawler = BingImageCrawler(storage={\"root_dir\": folder})\n",
        "    crawler.crawl(keyword=keyword, max_num=max_num)\n",
        "    print(f\"Downloaded {max_num} images into '{folder}'.\")\n",
        "\n",
        "fetch_images_bing(\"apple fruit\", \"apple\", 50)\n",
        "print(\"Download complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhwNb93yQpWx",
        "outputId": "7499545e-7cdb-44e6-c4c2-5df8468be688"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for 'apple fruit' via Bing …\n",
            "Downloaded 50 images into 'apple'.\n",
            "Download complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IZavhIji8cDR",
        "outputId": "f96535dc-88af-429d-9fdf-e53a8059de30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/apple/\n",
            "  inflating: dataset/.DS_Store       \n",
            "  inflating: __MACOSX/dataset/._.DS_Store  \n",
            "   creating: dataset/orange/\n",
            "  inflating: dataset/apple/AdobeStock_840698529_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_840698529_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_1209170540_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_1209170540_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_59967553_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_59967553_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_539609028_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_539609028_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_176979696_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_176979696_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_1522294100_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_1522294100_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_1413022968_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_1413022968_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_598818528_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_598818528_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_169233743_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_169233743_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_91125913_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_91125913_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_602163211_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_602163211_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_965779099_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_965779099_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_569363672_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_569363672_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_965809879_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_965809879_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_48470256_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_48470256_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_526739981_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_526739981_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_567825579_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_567825579_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_1484627415_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_1484627415_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_277584211_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_277584211_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_847964258_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_847964258_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_139579875_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_139579875_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_739765918_Preview.png  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_739765918_Preview.png  \n",
            "  inflating: dataset/apple/AdobeStock_111105580_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_111105580_Preview.jpeg  \n",
            "  inflating: dataset/apple/AdobeStock_72697530_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/apple/._AdobeStock_72697530_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_542465345_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_542465345_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_195834894_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_195834894_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_188617296_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_188617296_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_622218280_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_622218280_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_559491459_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_559491459_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_279120239_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_279120239_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_93864955_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_93864955_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_567655769_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_567655769_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_594328339_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_594328339_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_525179072_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_525179072_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_523952669_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_523952669_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_86627021_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_86627021_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_77068636_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_77068636_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_61654083_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_61654083_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_56010077_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_56010077_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_201795443_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_201795443_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_787861650_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_787861650_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_791032505_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_791032505_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_529584615_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_529584615_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_529270830_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_529270830_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_556342665_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_556342665_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_102310122_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_102310122_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_806205677_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_806205677_Preview.png  \n",
            "  inflating: dataset/orange/AdobeStock_41068624_Preview.jpeg  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_41068624_Preview.jpeg  \n",
            "  inflating: dataset/orange/AdobeStock_602412323_Preview.png  \n",
            "  inflating: __MACOSX/dataset/orange/._AdobeStock_602412323_Preview.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move your personal apple images into the main apple folder\n",
        "!mv dataset/apple/* apple/\n",
        "\n",
        "# Move your personal orange images into the main orange folder\n",
        "!mv dataset/orange/* orange/\n",
        "\n",
        "print(\"Your personal images have been merged with the scraped images!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5n0zBKt8-a2m",
        "outputId": "6afb8ff5-2ae6-444d-b569-08b3347931e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your personal images have been merged with the scraped images!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2ensM5jE-t_3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple_count = len(os.listdir('apple'))\n",
        "orange_count = len(os.listdir('orange'))\n",
        "print(f\"We have {apple_count} apple images.\")\n",
        "print(f\"We have {orange_count} orange images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQYSOy2R__Vs",
        "outputId": "6ce72797-4068-4058-f16b-fc29916d65e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 74 apple images.\n",
            "We have 75 orange images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_apple_file = os.listdir('apple')[0]\n",
        "image_path = os.path.join('apple', first_apple_file)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XpBHF7LEAJyR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(image_path)\n",
        "print(\"Image details:\")\n",
        "print(f\" - Format: {img.format}\")\n",
        "print(f\" - Size (Width x Height): {img.size}\")\n",
        "print(f\"  - Mode: {img.mode}\") # 'RGB' means Red, Green, Blue channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87h2VtxjAcDE",
        "outputId": "c9b50e0c-5110-4340-8975-6f1bfd27da3e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image details:\n",
            " - Format: JPEG\n",
            " - Size (Width x Height): (905, 1000)\n",
            "  - Mode: RGB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_as_array = np.array(img)\n",
        "print(img_as_array[:5,:5,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5bDKbmnBdR1",
        "outputId": "ded7ac41-8a27-4a3e-ae8e-ccc3cad7e8aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[255 255 255 255 255]\n",
            " [255 255 255 255 255]\n",
            " [255 255 255 255 255]\n",
            " [255 255 255 255 255]\n",
            " [255 255 255 255 255]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#REsizing"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eKacCdZHBput"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resized_img = img.resize((128,128))\n",
        "print(f\"The new size is: {resized_img.size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrLhVczoEJNt",
        "outputId": "677ae3cc-f0b0-45d3-a8c5-e645a270e5bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new size is: (128, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "MrNzNGmXEZ2v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer"
      ],
      "metadata": {
        "id": "eOeNgYSwEpOk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_tensor_transformer = T.ToTensor()\n",
        "img_tensor = to_tensor_transformer(resized_img)"
      ],
      "metadata": {
        "id": "MgjKipUxHXzS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The type of our new object is: {type(img_tensor)}\")\n",
        "print(f\"The shape of the tensor is: {img_tensor.shape}\") #x:rgb channels, y:height, z:width"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOHv5OsuHmbR",
        "outputId": "59efccd2-4b72-42bb-e807-d3e8906a39ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of our new object is: <class 'torch.Tensor'>\n",
            "The shape of the tensor is: torch.Size([3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets create a pipeline of transformers\n",
        "#First resize the iamge, then convert it to tensor"
      ],
      "metadata": {
        "id": "Tv800htcHtO_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_pipeline = T.Compose([T.Resize((128,128)), T.ToTensor()])"
      ],
      "metadata": {
        "id": "TF9z16k2I5aI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!mv apple dataset/\n",
        "!mv orange dataset/\n",
        "\n",
        "print(\"Folders organized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-60bwkQeJFxn",
        "outputId": "65eb04ac-17c6-4b5f-a2da-48c3f88592f4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "Folders organized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "4dsGxGYiJQ6K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the dataset object\n",
        "full_dataset = ImageFolder(root='dataset', transform = transform_pipeline)\n"
      ],
      "metadata": {
        "id": "2KcXONlhJc3b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check if it worked\n",
        "print(f\"Found {len(full_dataset)} total images.\")\n",
        "print(f\"The classes (labels) are: {full_dataset.classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A4hn1oZJpWw",
        "outputId": "22569c7a-75df-4b9f-dd9e-4c925d1eb34e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 149 total images.\n",
            "The classes (labels) are: ['apple', 'orange']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train test splitting"
      ],
      "metadata": {
        "id": "0RinmhQhJqZ-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "eie5ju2cJ3oK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the sizes for our split(80% train and 20% test)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size"
      ],
      "metadata": {
        "id": "Dv-rPfc1J8HD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing the split\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])"
      ],
      "metadata": {
        "id": "rgtre2SJKJfB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of images in Training Set: {len(train_dataset)}\")\n",
        "print(f\"Number of images in Testing Set: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhzu_9OnKV1-",
        "outputId": "49259e53-8f72-49fd-b193-ad8c805a329c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in Training Set: 119\n",
            "Number of images in Testing Set: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "rGN52jXpKYCC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "metadata": {
        "id": "BuTnEYR1Wr3w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "NNsqHa6rWtzp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data loaders created!\")\n",
        "print(f\"The traning loader will serve data in batches of {batch_size}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNVq-tLuW9WN",
        "outputId": "445db473-98c5-434e-99ec-fd715feb3321"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaders created!\n",
            "The traning loader will serve data in batches of 16.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convulutional Neural Network"
      ],
      "metadata": {
        "id": "25fK2pcjXGqH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "vxc07d11ZBZW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Define first layer: it takes 3 input channels and produces 16 output feature maps\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n",
        "    # Define second layer: it takes. the 16 feature maps from conv1 and produces 32 new ones\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size = 3, padding = 1)\n",
        "    # Define a max pooling layer, it downsamples the image, keeping the important information\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    # Define the first fully connected (linear) layer: this starts the classification process\n",
        "    # The input size(32 * 32 * 32) is calculated from the image size after pooling\n",
        "    self.fc1 = nn.Linear(32 * 32 * 32, 256)\n",
        "    # Define the final output layer, it takes 256 inputs and produces 2 outputs (one score for apple and one score for orange)\n",
        "    self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "\n",
        "\n",
        "  #The function defines the forward pass, how data flows through the layers\n",
        "  def forward(self, x):\n",
        "    # Data -> conv1 -> activation -> pool\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    # Data -> conv2 -> activation -> pool\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    # Now flatten the 2d image into a 1d vector to feed into the linear layers\n",
        "    x = torch.flatten(x,1)\n",
        "    # Data -> fc1 -> activation\n",
        "    x = F.relu(self.fc1(x))\n",
        "    # Data -> fc2 -> activation\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Now lets create an instance of our model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Print the model to see its architecture\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttDr1H9aZHkn",
        "outputId": "8a3aad51-2956-4da0-9cd0-dffad2964658"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=32768, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's the blueprint of the \"brain\" you just created! Think of it as an assembly line for your image data.\n",
        "\n",
        "Here's what each part means, step-by-step:\n",
        "\n",
        "    conv1 (Convolutional Layer 1):\n",
        "\n",
        "        Job: The first worker on the line. It scans the original image (with its 3 color channels) and looks for 16 very basic patterns, like simple edges or color blobs.\n",
        "\n",
        "    conv2 (Convolutional Layer 2):\n",
        "\n",
        "        Job: Takes the 16 simple patterns from conv1 and combines them to look for 32 slightly more complex patterns, like curves or textures.\n",
        "\n",
        "    pool (Max Pooling Layer):\n",
        "\n",
        "        Job: A quality control step. It shrinks the pattern maps by half, keeping only the strongest signals. This makes the model more efficient and helps it focus on the most important features.\n",
        "\n",
        "    fc1 (Fully Connected Layer 1):\n",
        "\n",
        "        Job: The manager. It takes all the high-level pattern information, which has been flattened into a long list of numbers (in_features=32768), and starts the decision-making process, summarizing it down to 256 key indicators.\n",
        "\n",
        "    fc2 (Fully Connected Layer 2 - The Final Decision):\n",
        "\n",
        "        Job: The final boss. It takes the 256 indicators and makes the final call. It outputs 2 numbers. One number represents the model's confidence that the image is an 'apple', and the other is its confidence that it's an 'orange'. The higher number wins."
      ],
      "metadata": {
        "id": "YhcQvJPfcU2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOSS AND OPTIM"
      ],
      "metadata": {
        "id": "kDIcJM3BbfDq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "I0zUJ9R0cp8s"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss fucntion\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "60RXooukc5GS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optmizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "print(\"Loss and optim are ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO29ud4hc_PR",
        "outputId": "5f969fc4-3d90-4f93-cd64-9437783211e1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and optim are ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "lOR87BIPdMWT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the heart of the process. We will create a loop that does the following, over and over again:\n",
        "\n",
        "    Gets a batch of images and their correct labels from the train_loader.\n",
        "\n",
        "    Shows the images to the model and gets its predictions.\n",
        "\n",
        "    Compares the predictions to the correct labels using the criterion (loss function) to see how wrong it was.\n",
        "\n",
        "    Uses the optimizer to slightly adjust the model's internal connections to make it a little less wrong next time.\n",
        "\n",
        "We will repeat this process for all the batches in our training set. Going through the entire training set once is called an epoch. We will train for a few epochs."
      ],
      "metadata": {
        "id": "lXnnh3QqdwD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No. of epochs\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "UaKNUv_2dpsQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "for epoch in range(num_epochs):\n",
        "  # variable will track loss for epoch\n",
        "  running_loss = 0.0\n",
        "  # now Get batches of data from training loader\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # Data object contains a batch of [images, labels]\n",
        "    inputs, labels = data\n",
        "\n",
        "    # Step1: Zero the parameter gradients, this is a necessary reset for each batch.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Step2: Forward pass, feed the images(data) to the model to get its predictions(ouputs)\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Step3: calculate loss, compare the model's outputs to the corrrect labels.\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Step4: Backward pass, calculate how much each model param contributed to the error.\n",
        "    loss.backward()\n",
        "\n",
        "    # Step5: Optimize, update the models' param to reduce the loss.\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the loss for this batch to the running total\n",
        "    running_loss += loss.item()\n",
        "  #Print the avaerage loss for this epoch\n",
        "  print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "print(\"Finished training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WYmmr_Bdupn",
        "outputId": "ede78a35-0dd7-4d9f-849b-adbb5c6d518b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.2194396257400513\n",
            "Epoch 2, Loss: 0.5175292901694775\n",
            "Epoch 3, Loss: 0.2860301025211811\n",
            "Epoch 4, Loss: 0.1631574798375368\n",
            "Epoch 5, Loss: 0.15033314027823508\n",
            "Finished training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating our model based on ur test data"
      ],
      "metadata": {
        "id": "UGy5RzD4jBNS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0"
      ],
      "metadata": {
        "id": "5d1JmutlnlPy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to eval mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "408Xa1X8no33",
        "outputId": "56dccdfa-ff20-4e8c-8e53-efd16658c6d5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=32768, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to calculate gradients for evaluation, so we use torch.no_grad()\n",
        "# This makes the code run faster and use less memory.\n",
        "with torch.no_grad():\n",
        "  # loop over the batches in test loader\n",
        "  for data in test_loader:\n",
        "    # Get images, and their true labels from the data batch\n",
        "    images, labels = data\n",
        "    # Feed the images to the model\n",
        "    outputs = model(images)\n",
        "    # The model outputs a score for each class. We want the class with the highest score.\n",
        "    # torch.max returns the value and the index of the highest score. We only need the index.\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    # Now, add the no. of images in this batch to our total count\n",
        "    total += labels.size(0)\n",
        "    # Add the no. of correct predictions in this batch to our running total\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy of the model on the {len(test_dataset)} test images: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-8tcVjsntaj",
        "outputId": "c69d67ed-3ea7-463a-89f2-14d169bebad9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the 30 test images: 75.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEO0SO-PoxrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}